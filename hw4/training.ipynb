{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIo1kATHW0Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcMzzO4EXFHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp drive/My\\ Drive/11785/hw4_p1/hw4/tests.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIvmxBTzWwhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests import test_prediction, test_generation\n",
        "from tqdm.notebook import tqdm # TODO: delete thie before submit\n",
        "import time\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tnF-TYyWwhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all that we need\n",
        "\n",
        "root = \"drive/My Drive/11785/hw4_p1/\"\n",
        "dataset = np.load(root + 'dataset/wiki.train.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load(root + 'fixtures/prediction.npz', allow_pickle=True)  # dev\n",
        "fixtures_gen = np.load(root + 'fixtures/generation.npy', allow_pickle=True)  # dev\n",
        "fixtures_pred_test = np.load(root + 'fixtures/prediction_test.npz', allow_pickle=True)  # test\n",
        "fixtures_gen_test = np.load(root + 'fixtures/generation_test.npy', allow_pickle=True)  # test\n",
        "vocab = np.load(root + 'dataset/vocab.npy', allow_pickle=True)\n",
        "\n",
        "vocab_map = {idx: i for idx, i in enumerate(vocab)}\n",
        "def index2sentence(tokens):\n",
        "    return \" \".join([vocab_map[token.item()] for token in tokens])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIK9BCcqWwhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader():\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.seq_len = 200\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate your articles and build into batches\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "        \n",
        "        dataset_cat = np.concatenate(self.dataset)\n",
        "        n_seq = len(dataset_cat) // self.seq_len\n",
        "        dataset_cat = dataset_cat[:n_seq * self.seq_len]\n",
        "        data = torch.LongTensor(dataset_cat).view(-1, self.seq_len)\n",
        "        \n",
        "        for i in range(0, len(data), self.batch_size):\n",
        "            batch_data = data[i:i+self.batch_size]\n",
        "            yield (batch_data[:, :-1], batch_data[:, 1:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEekkZ4xWwhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "# Reference: https://github.com/salesforce/awd-lstm-lm\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = 3\n",
        "        self.input_size = 128\n",
        "        self.output_size = vocab_size\n",
        "        self.weight_tying = True\n",
        "        self.hidden_size = self.input_size if self.weight_tying else 128\n",
        "        self.dropouti = 0.4\n",
        "        self.dropouth = 0.3\n",
        "        self.dropout = 0.4\n",
        "        self.dropoute = 0.1\n",
        "\n",
        "        self.word_embedding = nn.Embedding(self.vocab_size, self.input_size)\n",
        "        self.lstms = nn.LSTM(input_size=self.input_size, \n",
        "                            hidden_size=self.hidden_size,\n",
        "                            num_layers=self.num_layers,\n",
        "                            dropout=0,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "        if self.weight_tying:\n",
        "            self.word_embedding.weight = self.decoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        print(self.lstms)\n",
        "        # TODO: initialization of all weights\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.word_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        batch_size = x.size(0)        \n",
        "        # x.shape: [batch_size, seq_len]\n",
        "\n",
        "        x = self.word_embedding(x)\n",
        "        # x.shape: [batch_size, seq_len, input_size]\n",
        "\n",
        "        x, _ = self.lstms(x)\n",
        "        # x.shape: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        x = x.contiguous()\n",
        "        x = self.decoder(x)\n",
        "        # x.shape: [batch_size, seq_len, vocab_size]\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH94I6kiWwhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model.to(device)\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        self.clip = 0.25\n",
        "\n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                         lr=5, \n",
        "                                         weight_decay=1.2e-6) # TODO: try NT-ASGD\n",
        "        self.criterion = torch.nn.CrossEntropyLoss() # TODO: use splitCrossEntropy\n",
        "\n",
        "        self.params = list(self.model.parameters()) + list(self.criterion.parameters())\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        with tqdm(total=10500/self.loader.batch_size) as pbar:\n",
        "            for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "                epoch_loss += self.train_batch(inputs, targets)\n",
        "                pbar.update(1)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "        \n",
        "        \"\"\"\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = self.model(inputs) \n",
        "        \n",
        "        loss = self.criterion(outputs.view(-1, outputs.size(2)), targets.contiguous().view(-1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if self.clip:\n",
        "            torch.nn.utils.clip_grad_norm_(self.params, self.clip)\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self, exp_path=None):\n",
        "        # TODO: change this back\n",
        "        if exp_path:\n",
        "            model_path = os.path.join(exp_path, self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "            torch.save({'state_dict': self.model.state_dict()},\n",
        "                model_path)\n",
        "            np.save(os.path.join(exp_path, self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "            np.save(os.path.join(exp_path, self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "            np.save(os.path.join(exp_path, self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "            np.save(os.path.join(exp_path, self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "            with open(os.path.join(exp_path, self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "                fw.write(self.generated[-1])\n",
        "            with open(os.path.join(exp_path, self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "                fw.write(self.generated_test[-1])\n",
        "        else:\n",
        "            # don't change these\n",
        "            model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "            torch.save({'state_dict': self.model.state_dict()},\n",
        "                model_path)\n",
        "            np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "            np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "            np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "            np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "            with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "                fw.write(self.generated[-1])\n",
        "            with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "                fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFC02HOCWwhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "        inp = torch.LongTensor(inp).to(device)\n",
        "        outputs = model(inp)\n",
        "        return outputs[:, -1, :].detach().cpu().numpy()\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\" \n",
        "        # TODO: change this to adapt to new arch\n",
        "        model.to(device)\n",
        "        inp = torch.LongTensor(inp).to(device)\n",
        "        generated_words = []\n",
        "                \n",
        "        embed = model.word_embedding(inp)\n",
        "        output_lstm, hidden = model.lstms(embed)\n",
        "        scores = model.decoder(output_lstm[:, -1, :])\n",
        "        _, current_word = torch.max(scores, dim=1)\n",
        "        generated_words.append(current_word)\n",
        "        \n",
        "        for i in range(forward-1):\n",
        "            embed = model.word_embedding(current_word).unsqueeze(1)\n",
        "            output_lstm, hidden = model.lstms(embed, hidden)\n",
        "            scores = model.decoder(output_lstm[:, -1, :])\n",
        "            _, current_word = torch.max(scores, dim=1)\n",
        "            generated_words.append(current_word)\n",
        "        \n",
        "        predicts = torch.stack(generated_words).permute(1, 0)\n",
        "        return predicts.detach().cpu().numpy()        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knNBlhZJWwhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 40\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRQDDrFeWwhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_id = str(int(time.time()))\n",
        "exp_path = root + \"hw4/experiments\"\n",
        "if not os.path.exists(exp_path):\n",
        "    os.mkdir(exp_path)\n",
        "os.mkdir(exp_path + '/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to \" + exp_path + run_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjTodhVvWwhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LanguageModel(len(vocab))\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HCQIpznSvBPt",
        "colab": {}
      },
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save(exp_path) # TODO: change this back\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMZVqfVEWwh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.savefig(\"curve.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjRZ7YGXWwh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}